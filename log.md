# 100 Days Of Code - Log

I started this challenge in an effort to build better habits and pursue my interests in coding. My plan going forward is to continue my research on neural networks and look into contributing to open source projects.

### Day 1: July 31, 2019

**Today's Progress**: Worked on understanding neural networks in terms of matrices. I believe I have worked out back propagation for my RGB text predictor, more testing tomorrow.

**Thoughts:**: Going from the simple network used in the flower problem to the one used in the RGB text predictor was incredibly challenging, scaling further shouldn't require too much effort to figure out.

**Link to work:** [Color Picker](https://github.com/jpeter17/color_picker_NN)

### Day 2: Aug 1, 2019

**Today's Progress**: Finished the model for the RGB color predictor and moved on to adding information to the UI. 

**Thoughts:**: Formatting numpy matrices took some doing but wasn't too difficult to figure out. I can't tell if the network is functioning properly. Using a labeled dataset in the next project would make this more clear. 

**Link to work:** [Color Picker](https://github.com/jpeter17/color_picker_NN)

### Day 3: Aug 2, 2019

**Today's Progress**: Did some research on the reasoning behind number of hidden layers and nodes within them. Started a new network, dubbed 'Skribble', that will eventually be able to interpret handwritten digits for the MNIST data set.

**Thoughts:**: Did not make as much progress as I wanted to on this. I did my coding on a bus to Portland and did not think to eat before hand. The information I learned on hidden layers and neurons was very interesting.

**Link to work:** [Skribble](https://github.com/jpeter17/Skribble)

### Day 4: Aug 5, 2019

**Today's Progress**: Updated some things in the Skribble Network and found some bugs in calculating the change in weight that I need to keep working on. 

**Thoughts:**: Extremes of the sigmoid function return a derivative of zero. This is causing me some issues in change in weight. I need to look into changing the activation function to something that doesnt have a derivative tending to 0 or make sure my z-values stay relatively close to 0 

**Link to work:** [Skribble](https://github.com/jpeter17/Skribble)

### Day 5: Aug 6, 2019

**Today's Progress**: Worked on some questions from previous rounds of Google Code Jam and LeetCode. Completed first 4 modules of Google's Machine Learning Crash Course. 

**Thoughts:**: I have been looking into signing up for the upcoming Google Kick Start Competition. I will work on past problems in the upcoming days to decide. The Machine Learning crash course has introduced me to some new technologies that will be useful in the Skribble network as well as much more memory friendly ways to visualize data.

### Day 6: Aug 7, 2019

**Today's Progress**: Did problems from previous rounds of Google's Kick Start competition to prepare for the upcoming round. Additionally completed some more modules in the machine learning crash course.

**Thoughts:**: I know that I have the ability to score at least a few points in Kick Start so I figure I might as well participate. I have picked up a few books to hone my algorithm and data structure ability in preperation. 

### Day 7: Aug 8, 2019

**Today's Progress**: More modules from the machine learning crash course. Learned about bucketization of data and it's usefulness in addition to the FTRL optimizer. The results are impressive. 

**Thoughts:**: Bucketization might assist me with the accuracy of the Skribble Network. Binning the brightness values into fewer groups such as (dark, dim, bright) might increase accuracy. FTLR optimization will allow adjustment of learning rates for the outermost pixels that will almost always be classified as dark. 

### Day X: 

**Today's Progress**: 

**Thoughts:**: 

**Link to work:** [Skribble](https://github.com/jpeter17/Skribble)


